{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cb04893-d893-419c-bfd0-8bd330eacc36",
   "metadata": {},
   "source": [
    "# **AI Python For Beginners**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5231f3-101c-4218-95c2-0f303e7c16bc",
   "metadata": {},
   "source": [
    "## **Lesson 3: Navigate the learning platform**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5ab7063-d47d-42ee-b6c3-8692ebda1e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World :)\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello World :)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3075f603-4580-4fd8-a237-d3c15a3eaba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 292 days betweem christmas and my birthday\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "christmas = datetime(year=2024,month=12,day=25)\n",
    "birthday = datetime(year=2025,month=10,day=13)\n",
    "delta = birthday-christmas\n",
    "print(f\"there are {delta.days} days betweem christmas and my birthday\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795c4a36",
   "metadata": {},
   "source": [
    "# **Multi-line f-string**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b43ff73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Most countries use the metric system por recipe measurment,\n",
      "        but American bakers use a different system. For example, they se fluid\n",
      "        ounces to measure liquits insted of milliliters(ml).\n",
      "        So you need to convert recipe units to your local measuring system!\n",
      "        For example. 8 fluid ounces of mild is 236.588 ml.\n",
      "        And 100 ml of water is 3.381405650328842 fluid ounces.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "        Most countries use the metric system por recipe measurment,\n",
    "        but American bakers use a different system. For example, they se fluid\n",
    "        ounces to measure liquits insted of milliliters(ml).\n",
    "        So you need to convert recipe units to your local measuring system!\n",
    "        For example. 8 fluid ounces of mild is {8*29.5735} ml.\n",
    "        And 100 ml of water is {100/29.5735} fluid ounces.\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd48ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import print_llm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0285551",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%writefile` not found.\n"
     ]
    }
   ],
   "source": [
    "# @title test.py\n",
    "%%writefile test.py\n",
    "import sys\n",
    "print(sys.argv)\n",
    "print(\"Hola, mi nombre es\", sys.argv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "335928ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"helper_functions.py\", \"w\") as f:\n",
    "    f.write(\"\"\"\n",
    "def print_llm_response(response):\n",
    "    if isinstance(response, dict) and \"choices\" in response:\n",
    "        print(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "    elif isinstance(response, str):\n",
    "        print(response)\n",
    "    else:\n",
    "        print(response)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cfce46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahora funciona en Jupyter Desktop üöÄ\n"
     ]
    }
   ],
   "source": [
    "from helper_functions import print_llm_response\n",
    "print_llm_response(\"Ahora funciona en Jupyter Desktop üöÄ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6fcccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#variables\n",
    "number_of_lines=5\n",
    "# prompt\n",
    "prompt = f\"\"\"Write a comedy story in {number_of_lines} lines\"\"\"\n",
    "#response\n",
    "response = get_llm_response(prompt)\n",
    "# respuesta\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f3c89e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_llm_response' from 'helper_functions' (C:\\Users\\soporte\\Documents\\DEEPLEARNING_AI\\AI_PYTHON\\helper_functions.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhelper_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m print_llm_response,get_llm_response\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'get_llm_response' from 'helper_functions' (C:\\Users\\soporte\\Documents\\DEEPLEARNING_AI\\AI_PYTHON\\helper_functions.py)"
     ]
    }
   ],
   "source": [
    "from helper_functions import print_llm_response,get_llm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "738bde10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Write a four line birthday poem for my frined Daniel.\n",
      "The poem should be inspired by the first letter of my friend's name.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "name = \"Daniel\"\n",
    "prompt=f\"\"\"\n",
    "Write a four line birthday poem for my frined {name}.\n",
    "The poem should be inspired by the first letter of my friend's name.\n",
    "\"\"\"\n",
    "print_llm_response(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9031adcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_task=[\n",
    "    \"Compose a brief email to my boss\",\n",
    "    \"Write a birthday poerm for Otto\",\n",
    "    \"Write a 300-word review of the movil 'The Arrival'\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9130c6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose a brief email to my boss\n"
     ]
    }
   ],
   "source": [
    "task = list_of_task[0]\n",
    "print_llm_response(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66a823be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ice cream flavor example\n",
    "ice_cream_flavors=[\n",
    "    \"Vanilla\",\n",
    "    \"Chocolate\",\n",
    "    \"Strawberry\",\n",
    "    \"Mint Chocolate Chip\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84ec2a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the ice cream flavor listed below, \n",
      "    provide a captivating description thtat could be used for promotion\n",
      "    Flavor: Vanilla\n",
      "    \n",
      "For the ice cream flavor listed below, \n",
      "    provide a captivating description thtat could be used for promotion\n",
      "    Flavor: Chocolate\n",
      "    \n",
      "For the ice cream flavor listed below, \n",
      "    provide a captivating description thtat could be used for promotion\n",
      "    Flavor: Strawberry\n",
      "    \n",
      "For the ice cream flavor listed below, \n",
      "    provide a captivating description thtat could be used for promotion\n",
      "    Flavor: Mint Chocolate Chip\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "for flavor in ice_cream_flavors:\n",
    "    prompt=f\"\"\"For the ice cream flavor listed below, \n",
    "    provide a captivating description thtat could be used for promotion\n",
    "    Flavor: {flavor}\n",
    "    \"\"\"\n",
    "    print_llm_response(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b494e6bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_llm_response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 7\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m flavor \u001b[38;5;129;01min\u001b[39;00m ice_cream_flavors:\n\u001b[0;32m      3\u001b[0m     prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mFor the ice cream flavor listed bellow,\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124m    provide a captivating description that could be used for promotion\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124m    Flavor: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mflavor\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m----> 7\u001b[0m     description\u001b[38;5;241m=\u001b[39m get_llm_response(prompt)\n\u001b[0;32m      8\u001b[0m     promotional_description\u001b[38;5;241m.\u001b[39mappend(description)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_llm_response' is not defined"
     ]
    }
   ],
   "source": [
    "promotonal_descriptions=[]\n",
    "for flavor in ice_cream_flavors:\n",
    "    prompt=f\"\"\"For the ice cream flavor listed bellow,\n",
    "    provide a captivating description that could be used for promotion\n",
    "    Flavor: {flavor}\n",
    "    \"\"\"\n",
    "    description= get_llm_response(prompt)\n",
    "    promotional_description.append(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e2c6429",
   "metadata": {},
   "outputs": [],
   "source": [
    "#task example, large list not ordered by priority. Want to prioritize\n",
    "list_of_tasks = [\n",
    "    \"Compose a brief email to my boss explaining that I will be late for tomorrow's meeting.\",\n",
    "    \"Write a birthday poem for Otto, celebrating his 28th birthday.\",\n",
    "    \"Write a 300-word review of the movie 'The Arrival'.\",\n",
    "    \"Draft a thank-you note for my neighbor Dapinder who helped water my plants while I was on vacation.\",\n",
    "    \"Create an outline for a presentation on the benefits of remote work.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36487c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instead of that unorganized large list, divide tasks by priority\n",
    "high_priority_tasks = [\n",
    "    \"Compose a brief email to my boss explaining that I will be late for tomorrow's meeting.\",\n",
    "    \"Create an outline for a presentation on the benefits of remote work.\"\n",
    "]\n",
    "\n",
    "medium_priority_tasks = [\n",
    "    \"Write a birthday poem for Otto, celebrating his 28th birthday.\",\n",
    "    \"Draft a thank-you note for my neighbor Dapinder who helped water my plants while I was on vacation.\"\n",
    "]\n",
    "\n",
    "low_priority_tasks = [\n",
    "    \"Write a 300-word review of the movie 'The Arrival'.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c981f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dictionary with all tasks\n",
    "#dictionaries can contain lists!\n",
    "prioritized_tasks = {\n",
    "    \"high_priority\": high_priority_tasks,\n",
    "    \"medium_priority\": medium_priority_tasks,\n",
    "    \"low_priority\": low_priority_tasks\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ac87f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#complete high priority tasks \n",
    "for task in prioritized_tasks[\"high_priority\"]:\n",
    "    print_llm_response(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ee0346",
   "metadata": {},
   "outputs": [],
   "source": [
    "food_preferences_tommy = {\n",
    "        \"dietary_restrictions\": \"vegetarian\",\n",
    "        \"favorite_ingredients\": [\"tofu\", \"olives\"],\n",
    "        \"experience_level\": \"intermediate\",\n",
    "        \"maximum_spice_level\": 6\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c3cee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"Please suggest a recipe that tries to include \n",
    "the following ingredients: \n",
    "{food_preferences_tommy[\"favorite_ingredients\"]}.\n",
    "The recipe should adhere to the following dietary restrictions:\n",
    "{food_preferences_tommy[\"dietary_restrictions\"]}.\n",
    "The difficulty of the recipe should be: \n",
    "{food_preferences_tommy[\"experience_level\"]}\n",
    "The maximum spice level on a scale of 10 should be: \n",
    "{food_preferences_tommy[\"maximum_spice_level\"]} \n",
    "Provide a two sentence description.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64ece740",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import print_llm_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9a19927",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_list=[\n",
    "    {\"description\":\"a\",\n",
    "    \"time_to_complete\":3\n",
    "    },\n",
    "    {\"description\":\"b\",\n",
    "    \"time_to_complete\":60\n",
    "    },\n",
    "    {\"description\":\"c\",\n",
    "    \"time_to_complete\":30\n",
    "    },\n",
    "    {\"description\":\"d\",\n",
    "    \"time_to_complete\":5\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1426dce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'description': 'a', 'time_to_complete': 3}\n"
     ]
    }
   ],
   "source": [
    "task= task_list[0]\n",
    "print(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "754d0dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "if task[\"time_to_complete\"]<=5:\n",
    "    task_to_do = task[\"description\"]\n",
    "    print_llm_response(task_to_do)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67119c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = task_list[1]\n",
    "if task[\"time_to_complete\"]<=5:\n",
    "    task_to_do = task[\"description\"]\n",
    "    print_llm_response(task_to_do)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "03f2fa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = task_list[2]\n",
    "if task[\"time_to_complete\"]<=5:\n",
    "    task_to_do = task[\"description\"]\n",
    "    print_llm_response(task_to_do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "034c18f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d\n"
     ]
    }
   ],
   "source": [
    "task = task_list[3]\n",
    "if task[\"time_to_complete\"]<=5:\n",
    "    task_to_do = task[\"description\"]\n",
    "    print_llm_response(task_to_do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39737630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "d\n"
     ]
    }
   ],
   "source": [
    "for task in task_list:\n",
    "    if task[\"time_to_complete\"]<=5:\n",
    "        task_to_do=task[\"description\"]\n",
    "        print_llm_response(task_to_do)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a66010",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#F5C780; padding:15px\"> ü§ñ <b>Use the Chatbot</b>:\n",
    "    <br><br>\n",
    "    Explain this code line by line:\n",
    "    <br><br>f = open(\"email.txt\", \"r\")\n",
    "    <br>email = f.read()\n",
    "    <br>f.close()\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057e40f1",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#F5C780; padding:15px\"> ü§ñ <b>Use the Chatbot</b>:\n",
    "    <br><br>\n",
    "    What happens if I don't close a file?\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf7eb42",
   "metadata": {},
   "source": [
    "## USING LLMs to extract bullet point from email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "594fc757",
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhelper_functions_original\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m read_journal\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m display, Markdown\n",
      "File \u001b[1;32m~\\Documents\\DEEPLEARNING_AI\\AI_PYTHON\\helper_functions_original.py:16\u001b[0m\n\u001b[0;32m     13\u001b[0m openai_api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Set up the OpenAI client\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m client \u001b[38;5;241m=\u001b[39m OpenAI(api_key\u001b[38;5;241m=\u001b[39mopenai_api_key)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_csv_dict\u001b[39m(csv_file_path):\n\u001b[0;32m     20\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This function takes a csv file and loads it as a dict.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\_client.py:135\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[1;34m(self, api_key, organization, project, webhook_secret, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[0;32m    133\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 135\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[0;32m    136\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    137\u001b[0m     )\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(api_key):\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "from helper_functions_original import read_journal\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8ba180",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"email.txt\", \"r\")\n",
    "email = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b3d230",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"Extract bullet points from the following email. \n",
    "Include the sender information. \n",
    "\n",
    "Email:\n",
    "{email}\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f2d1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bullet_points = get_llm_response(prompt)\n",
    "print(bullet_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15bd23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print in Markdown format\n",
    "display(Markdown(bullet_points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4adc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"cape_town.txt\", \"madrid.txt\", \"rio_de_janeiro.txt\", \n",
    "         \"sydney.txt\", \"tokyo.txt\"]\n",
    "\n",
    "for file in files:\n",
    "    # Read journal file for the city\n",
    "    f = open(file, \"r\")\n",
    "    journal = f.read()\n",
    "    f.close()\n",
    "\n",
    "    # TRY CHANGING THIS PROMPT TO ASK DIFFERENT QUESTIONS\n",
    "    prompt = f\"\"\"Respond with \"Yes\" or \"No\": \n",
    "    the journal describes restaurants and food dishes. \n",
    "\n",
    "    Journal:\n",
    "    {journal}\"\"\"\n",
    "\n",
    "    # Use LLM to determine if the journal entry is useful\n",
    "    print(f\"{file} -> {get_llm_response(prompt)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b3b3141f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import *\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c22a4604",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'read_journal' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m journal_rio_de_janeiro \u001b[38;5;241m=\u001b[39m read_journal(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrio_de_janeiro.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'read_journal' is not defined"
     ]
    }
   ],
   "source": [
    "journal_rio_de_janeiro = read_journal(\"rio_de_janeiro.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d82b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"cape_town.txt\", \"istanbul.txt\", \"new_york.txt\", \"paris.txt\", \n",
    "          \"rio_de_janeiro.txt\", \"sydney.txt\", \"tokyo.txt\"]\n",
    "\n",
    "for file in files:\n",
    "    #Open file and read contents\n",
    "    journal_entry = read_journal(file)\n",
    "\n",
    "    #Extract restaurants and display csv\n",
    "    prompt =  f\"\"\"Please extract a comprehensive list of the restaurants \n",
    "    and their respective best dishes mentioned in the following journal entry. \n",
    "    \n",
    "    Ensure that each restaurant name is accurately identified and listed. \n",
    "    Provide your answer in CSV format, ready to save.\n",
    "\n",
    "    Exclude the \"```csv\" declaration, don't add spaces after the \n",
    "    comma, include column headers.\n",
    "\n",
    "    Format:\n",
    "    Restaurant, Dish\n",
    "    Res_1, Dsh_1\n",
    "    ...\n",
    "\n",
    "    Journal entry:\n",
    "    {journal_entry}\n",
    "    \"\"\"\n",
    "    \n",
    "    print(file)\n",
    "    print_llm_response(prompt)\n",
    "    print(\"\") # Prints a blank line!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a84e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from helper_functions import get_llm_response, print_llm_response, display_table\n",
    "from IPython.display import Markdown\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82dcec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"itinerary.csv\", 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be083df",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_reader = csv.DictReader(f)\n",
    "itinerary = []\n",
    "for row in csv_reader:\n",
    "    print(row)\n",
    "    itinerary.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "58a727bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'f' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m f\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'f' is not defined"
     ]
    }
   ],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1628a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the first destination from the itinerary list (Hint: index=0)\n",
    "trip_stop = itinerary[0]\n",
    "print(trip_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16cad2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "city = trip_stop[\"City\"]\n",
    "country = trip_stop[\"Country\"]\n",
    "arrival = trip_stop[\"Arrival\"]\n",
    "departure = trip_stop[\"Departure\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fdb271",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"I will visit {city}, {country}, from {arrival} to {departure}. \n",
    "Please create a detailed daily itinerary.\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419491cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the LLM response\n",
    "response = get_llm_response(prompt)\n",
    "\n",
    "# Print in Markdown format\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b82b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_journal(file):\n",
    "    f = open(file, \"r\")\n",
    "    journal = f.read()\n",
    "    f.close()\n",
    "    print(journal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6df4978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_journal(file):\n",
    "    f = open(file, \"r\")\n",
    "    journal = f.read()\n",
    "    f.close()\n",
    "    # print(journal)\n",
    "    return journal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a166f786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fahrenheit_to_celsius(fahrenheit):\n",
    "    # Calculation for getting the temperature in celsius\n",
    "    celsius = (fahrenheit - 32) * 5 / 9\n",
    "    # Print the results\n",
    "    print(f\"{fahrenheit}¬∞F is equivalent to {celsius:.2f}¬∞C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4848f7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import get_llm_response\n",
    "\n",
    "def create_bullet_points(file):\n",
    "    # Complete code below to read in the file and store the contents as a string\n",
    "    f = open(file, \"r\")\n",
    "    file_contents = f.read()\n",
    "    f.close()\n",
    "    # Write a prompt and pass to an LLM\n",
    "    prompt = f\"\"\"create a three bullet point summary, and returns the bullets as a string. \n",
    "    In the following  file_contents:\n",
    "    {file_contents}\n",
    "    \"\"\"\n",
    "    bullets = get_llm_response(prompt) # Don't forget to add your prompt!\n",
    "\n",
    "    # Return the bullet points\n",
    "    return bullets\n",
    "\n",
    "# This line of code runs your function for istanbul.txt and returns the output\n",
    "output_bullets = create_bullet_points(\"istanbul.txt\")\n",
    "\n",
    "# Print the fucntion output\n",
    "print(output_bullets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4f5ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(file):\n",
    "    f = open(file, \"r\")\n",
    "    \n",
    "    csv_reader = csv.DictReader(f)\n",
    "    data = []\n",
    "    for row in csv_reader:\n",
    "        data.append(row)\n",
    "    f.close()\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d9d30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the itinerary.csv file\n",
    "itinerary = read_csv(\"itinerary.csv\")\n",
    "\n",
    "# Display the itinerary\n",
    "display_table(itinerary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b082c48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function called 'read_journal'\n",
    "def read_journal(journal_file):\n",
    "    f = open(journal_file, \"r\")\n",
    "    journal = f.read() \n",
    "    f.close()\n",
    "\n",
    "    # Return the journal content\n",
    "    return journal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24e9486",
   "metadata": {},
   "outputs": [],
   "source": [
    "journal = read_journal(\"sydney.txt\")\n",
    "\n",
    "print(journal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ec0f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the prompt\n",
    "prompt = f\"\"\"Please extract a comprehensive list of the restaurants \n",
    "and their respective specialties mentioned in the following journal entry. \n",
    "Ensure that each restaurant name is accurately identified and listed. \n",
    "Provide your answer in CSV format, ready to save. \n",
    "Exclude the \"```csv\" declaration, don't add spaces after the comma, include column headers.\n",
    "\n",
    "Format:\n",
    "Restaurant, Specialty\n",
    "Res_1, Sp_1\n",
    "...\n",
    "\n",
    "Journal entry:\n",
    "{journal}\n",
    "\"\"\"\n",
    "\n",
    "# Print the prompt\n",
    "print_llm_response(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18017139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the prompt\n",
    "prompt = f\"\"\"Please extract a comprehensive list of the restaurants \n",
    "and their respective specialties mentioned in the following journal entry. \n",
    "Ensure that each restaurant name is accurately identified and listed. \n",
    "Provide your answer in CSV format, ready to save. \n",
    "Exclude the \"```csv\" declaration, don't add spaces after the comma, include column headers.\n",
    "\n",
    "Format:\n",
    "Restaurant, Specialty\n",
    "Res_1, Sp_1\n",
    "...\n",
    "\n",
    "Journal entry:\n",
    "{journal}\n",
    "\"\"\"\n",
    "\n",
    "# Print the prompt\n",
    "print_llm_response(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9fd263ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fahrenheit_to_celsius(fahrenheit):\n",
    "    celsius= (fahrenheit-32)*5/9\n",
    "    print(f\"{fahrenheit}¬∞F is equivalent to {celsius:.2f}¬∞C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f21c0286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64¬∞F is equivalent to 17.78¬∞C\n"
     ]
    }
   ],
   "source": [
    "fahrenheit_to_celsius(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06100255",
   "metadata": {},
   "outputs": [],
   "source": [
    "spices = [\"cumin\", \"turmeric\", \"oregano\", \"paprika\"]\n",
    "vegetables = [\"lettuce\", \"tomato\", \"carrot\", \"broccoli\"]\n",
    "proteins = [\"chicken\", \"tofu\", \"beef\", \"fish\", \"tempeh\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75911eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_spices = sample(spices, 2)\n",
    "random_vegetables = sample(vegetables, 2)\n",
    "random_protein = sample(proteins, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6e5695",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"Please suggest a recipe that includes the following ingredients.\n",
    "\n",
    "Spices: {random_spices}\n",
    "Vegetables: {random_vegetables}\n",
    "Proteins: {random_protein}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c26b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f72469",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import get_llm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3899c107",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe = get_llm_response(prompt)\n",
    "\n",
    "print(recipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ba59f7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import requests # let's you download webpages into python\n",
    "from helper_functions import * \n",
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "39db0e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "# The url from one of the Batch's newsletter\n",
    "url = 'https://www.deeplearning.ai/the-batch/the-world-needs-more-intelligence/'\n",
    "\n",
    "# Getting the content from the webpage's contents\n",
    "response = requests.get(url)\n",
    "\n",
    "# Print the response from the requests\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "76b28e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soporte\\anaconda3\\Lib\\site-packages\\IPython\\core\\display.py:431: UserWarning: Consider using IPython.display.IFrame instead\n",
      "  warnings.warn(\"Consider using IPython.display.IFrame instead\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe src=https://www.deeplearning.ai/the-batch/the-world-needs-more-intelligence/ width=\"60%\" height=\"400\"></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(f'<iframe src={url} width=\"60%\" height=\"400\"></iframe>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2440cc7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ú® New course! Enroll in Knowledge Graphs for AI Agent API Discovery \n",
      "Dear friends,\n",
      "Last year, a number of large businesses and individuals went to the media and governments and pushed the message that AI is scary, impossible to control, and might even lead to human extinction. Unfortunately they succeeded: Now many people think AI is scary. But when I speak with regulators, media, and private citizens, I like to bring the issue of whether AI is beneficial or harmful back to a very basic question: Are we better off with more, or less, intelligence in the world?¬†\n",
      "Intelligence is the ability to apply skills and knowledge to make good decisions. Yes, intelligence can be used for nefarious purposes. But over many centuries, a major driver of civilization's progress has been people getting smarter and more educated. Until now, human intelligence has been the primary form of intelligence available. But with artificial intelligence, we have the opportunity to bring much more intelligence into the world. I discussed this opportunity in a recent¬†interview¬†(paywalled) with¬†Financial Times¬†reporter Ryan McMorrow.\n",
      "Historically, intelligence has been very expensive to acquire. It costs a lot to feed, raise, and train a broadly knowledgeable and experienced human being! That's why it‚Äôs so expensive to hire intelligence, such as a highly skilled doctor to examine and advise you on a medical condition, or a patient tutor who can understand your child and gently coach them where they need help. But with artificial intelligence, we have the potential to make intelligence cheap for everyone, so you no longer have to worry about a huge bill for seeing a doctor or educating your child.¬†\n",
      "For society's biggest problems, such as climate change, intelligence ‚Äî including artificial intelligence ‚Äî also has a significant role to play. While having more intelligence in the world isn't the only thing (there are also nuances such as how to share the wealth it creates, how it will affect jobs, and how to keep it from being used for evil purposes), I believe we are much better off as a society with more intelligence, be it human or artificial intelligence.¬†\n",
      "In my recent talk at TED AI (you can watch the 12-minute presentation¬†here), I touched on why I'm excited about AI and why I think many of the anxieties about it are misplaced. If you speak with someone who‚Äôs worried about AI, please forward the talk to them to see if it helps to reassure them. Or ask if they fundamentally believe we want more intelligence in the world. I find that answering this question can be a useful North Star for how we approach AI.\n",
      "Keep learning!\n",
      "Andrew\n",
      "P.S. Check out our new short course on ‚ÄúBuilding Applications with Vector Databases,‚Äù taught by Pinecone‚Äôs Tim Tully! Vector databases (DBs) are commonly associated with retrieval augmented generation (RAG) but actually have many uses in AI applications. In this course, you‚Äôll learn about (i) a basic semantic search app that uses a vector DB to find similar documents, (ii) a RAG application querying datasets it was not trained on, (iii) recommender systems that combine semantic search and RAG, (iv) hybrid search, which lets you work with dense and sparse vectors simultaneously, (v) anomaly detection applied to network logs, and (vi) an image-similarity application with a fun example that determines which parent a child resembles more. Come learn how you can use vector DBs to build many different types of applications!¬†Enroll here\n",
      "Stay updated with weekly AI News and Insights delivered to your inbox\n"
     ]
    }
   ],
   "source": [
    "# Using beautifulsoup to extract the text\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "# Find all the text in paragraph elements on the webpage\n",
    "all_text = soup.find_all('p')\n",
    "\n",
    "# Create an empty string to store the extracted text\n",
    "combined_text = \"\"\n",
    "\n",
    "# Iterate over 'all_text' and add to the combined_text string\n",
    "for text in all_text:\n",
    "    combined_text = combined_text + \"\\n\" + text.get_text()\n",
    "\n",
    "# Print the final combined text\n",
    "print(combined_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2e2e37",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#F5C780; padding:15px\"> ü§ñ <b>Use the Chatbot</b>:\n",
    "<br><br>\n",
    "What is the following code doing?\n",
    "<br><br>\n",
    "soup = BeautifulSoup(response.text, 'html.parser')<br>\n",
    "all_text = soup.find_all('p')\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d3ee6725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting aisetup\n",
      "  Downloading aisetup-0.1.9-py3-none-any.whl (6.8 kB)\n",
      "Collecting folium<0.18,>=0.17 (from aisetup)\n",
      "  Downloading folium-0.17.0-py2.py3-none-any.whl (108 kB)\n",
      "                                              0.0/108.4 kB ? eta -:--:--\n",
      "     -------------------------------------- 108.4/108.4 kB 6.1 MB/s eta 0:00:00\n",
      "Collecting ipython<9.0,>=8.18 (from aisetup)\n",
      "  Downloading ipython-8.37.0-py3-none-any.whl (831 kB)\n",
      "                                              0.0/831.9 kB ? eta -:--:--\n",
      "     -------------------                   440.3/831.9 kB 13.9 MB/s eta 0:00:01\n",
      "     ------------------------------------  829.4/831.9 kB 10.5 MB/s eta 0:00:01\n",
      "     -------------------------------------- 831.9/831.9 kB 8.7 MB/s eta 0:00:00\n",
      "Collecting ipywidgets<9.0.0,>=8.1.3 (from aisetup)\n",
      "  Downloading ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n",
      "                                              0.0/139.8 kB ? eta -:--:--\n",
      "     -------------------------------------- 139.8/139.8 kB 8.6 MB/s eta 0:00:00\n",
      "Collecting matplotlib<4.0.0,>=3.9.2 (from aisetup)\n",
      "  Downloading matplotlib-3.10.6-cp311-cp311-win_amd64.whl (8.1 MB)\n",
      "                                              0.0/8.1 MB ? eta -:--:--\n",
      "     -                                        0.3/8.1 MB 9.6 MB/s eta 0:00:01\n",
      "     ---                                      0.7/8.1 MB 9.5 MB/s eta 0:00:01\n",
      "     -----                                    1.1/8.1 MB 8.9 MB/s eta 0:00:01\n",
      "     --------                                 1.7/8.1 MB 9.9 MB/s eta 0:00:01\n",
      "     -----------                              2.3/8.1 MB 10.3 MB/s eta 0:00:01\n",
      "     --------------                           2.8/8.1 MB 10.7 MB/s eta 0:00:01\n",
      "     ----------------                         3.4/8.1 MB 11.0 MB/s eta 0:00:01\n",
      "     -------------------                      4.1/8.1 MB 11.3 MB/s eta 0:00:01\n",
      "     ----------------------                   4.6/8.1 MB 11.3 MB/s eta 0:00:01\n",
      "     -------------------------                5.2/8.1 MB 11.4 MB/s eta 0:00:01\n",
      "     ----------------------------             5.7/8.1 MB 11.5 MB/s eta 0:00:01\n",
      "     -------------------------------          6.3/8.1 MB 11.6 MB/s eta 0:00:01\n",
      "     ----------------------------------       6.9/8.1 MB 11.6 MB/s eta 0:00:01\n",
      "     ------------------------------------     7.5/8.1 MB 11.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  8.1/8.1 MB 11.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 8.1/8.1 MB 11.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: openai<2.0,>=1.42 in c:\\users\\soporte\\anaconda3\\lib\\site-packages (from aisetup) (1.107.0)\n",
      "Requirement already satisfied: pandas<3.0,>=2.2 in c:\\users\\soporte\\anaconda3\\lib\\site-packages (from aisetup) (2.3.0)\n",
      "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in c:\\users\\soporte\\anaconda3\\lib\\site-packages (from aisetup) (1.1.1)\n",
      "Collecting requests<3.0.0,>=2.32.3 (from aisetup)\n",
      "  Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "                                              0.0/64.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 64.7/64.7 kB 3.4 MB/s eta 0:00:00\n",
      "Collecting branca>=0.6.0 (from folium<0.18,>=0.17->aisetup)\n",
      "  Downloading branca-0.8.1-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: jinja2>=2.9 in c:\\users\\soporte\\anaconda3\\lib\\site-packages (from folium<0.18,>=0.17->aisetup) (3.1.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\soporte\\anaconda3\\lib\\site-packages (from folium<0.18,>=0.17->aisetup) (1.26.4)\n",
      "Requirement already satisfied: xyzservices in c:\\users\\soporte\\anaconda3\\lib\\site-packages (from folium<0.18,>=0.17->aisetup) (2022.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\soporte\\anaconda3\\lib\\site-packages (from ipython<9.0,>=8.18->aisetup) (0.4.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\soporte\\anaconda3\\lib\\site-packages (from ipython<9.0,>=8.18->aisetup) (4.4.2)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\soporte\\anaconda3\\lib\\site-packages (from ipython<9.0,>=8.18->aisetup) (0.18.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\soporte\\anaconda3\\lib\\site-packages (from ipython<9.0,>=8.18->aisetup) (0.1.6)\n",
      "Collecting prompt_toolkit<3.1.0,>=3.0.41 (from ipython<9.0,>=8.18->aisetup)\n",
      "  Downloading prompt_toolkit-3.0.52-py3-none-any.whl (391 kB)\n",
      "                                              0.0/391.4 kB ? eta -:--:--\n",
      "     ------------------------------------  389.1/391.4 kB 12.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- 391.4/391.4 kB 8.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\soporte\\anaconda3\\lib\\site-packages (from ipython<9.0,>=8.18->aisetup) (2.15.1)\n",
      "Requirement already satisfied: stack_data in c:\\users\\soporte\\anaconda3\\lib\\site-packages (from ipython<9.0,>=8.18->aisetup) (0.2.0)\n",
      "Collecting traitlets>=5.13.0 (from ipython<9.0,>=8.18->aisetup)\n",
      "  Downloading traitlets-5.14.3-py3-none-any.whl (85 kB)\n",
      "                                              0.0/85.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 85.4/85.4 kB ? eta 0:00:00\n",
      "Requirement already satisfied: typing_extensions>=4.6 in c:\\users\\soporte\\anaconda3\\lib\\site-packages (from ipython<9.0,>=8.18->aisetup) (4.15.0)\n",
      "Collecting comm>=0.1.3 (from ipywidgets<9.0.0,>=8.1.3->aisetup)\n",
      "  Downloading comm-0.2.3-py3-none-any.whl (7.3 kB)\n",
      "Collecting widgetsnbextension~=4.0.14 (from ipywidgets<9.0.0,>=8.1.3->aisetup)\n",
      "  Downloading widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n",
      "                                              0.0/2.2 MB ? eta -:--:--\n",
      "     --------                                 0.5/2.2 MB 15.5 MB/s eta 0:00:01\n",
      "     -------------------                      1.1/2.2 MB 13.5 MB/s eta 0:00:01\n",
      "     ------------------------------           1.7/2.2 MB 13.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.2/2.2 MB 12.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.2/2.2 MB 10.8 MB/s eta 0:00:00\n",
      "Collecting jupyterlab_widgets~=3.0.15 (from ipywidgets<9.0.0,>=8.1.3->aisetup)\n",
      "  Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl (216 kB)\n",
      "                                              0.0/216.6 kB ? eta -:--:--\n",
      "     ------------------------------------- 216.6/216.6 kB 12.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\soporte\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.9.2->aisetup) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\soporte\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.9.2->aisetup) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\soporte\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.9.2->aisetup) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\soporte\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.9.2->aisetup) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\soporte\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.9.2->aisetup) (23.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\soporte\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.9.2->aisetup) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\soporte\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.9.2->aisetup) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\soporte\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.9.2->aisetup) (2.8.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\soporte\\anaconda3\\lib\\site-packages (from openai<2.0,>=1.42->aisetup) (3.5.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\soporte\\anaconda3\\lib\\site-packages (from openai<2.0,>=1.42->aisetup) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\soporte\\anaconda3\\lib\\site-packages (from openai<2.0,>=1.42->aisetup) (0.26.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\soporte\\anaconda3\\lib\\site-packages (from openai<2.0,>=1.42->aisetup) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\soporte\\anaconda3\\lib\\site-packages (from openai<2.0,>=1.42->aisetup) (2.11.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\soporte\\anaconda3\\lib\\site-packages (from openai<2.0,>=1.42->aisetup) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\soporte\\anaconda3\\lib\\site-packages (from openai<2.0,>=1.42->aisetup) (4.65.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\soporte\\anaconda3\\lib\\site-packages (from pandas<3.0,>=2.2->aisetup) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\soporte\\anaconda3\\lib\\site-packages (from pandas<3.0,>=2.2->aisetup) (2024.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\soporte\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.32.3->aisetup) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\soporte\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.32.3->aisetup) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\soporte\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.32.3->aisetup) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\soporte\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.32.3->aisetup) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\soporte\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0,>=1.42->aisetup) (1.0.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\soporte\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0,>=1.42->aisetup) (0.14.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\soporte\\anaconda3\\lib\\site-packages (from jedi>=0.16->ipython<9.0,>=8.18->aisetup) (0.8.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\soporte\\anaconda3\\lib\\site-packages (from jinja2>=2.9->folium<0.18,>=0.17->aisetup) (2.1.1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\soporte\\anaconda3\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython<9.0,>=8.18->aisetup) (0.2.5)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\soporte\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai<2.0,>=1.42->aisetup) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\soporte\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai<2.0,>=1.42->aisetup) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\soporte\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai<2.0,>=1.42->aisetup) (0.4.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\soporte\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib<4.0.0,>=3.9.2->aisetup) (1.16.0)\n",
      "Requirement already satisfied: executing in c:\\users\\soporte\\anaconda3\\lib\\site-packages (from stack_data->ipython<9.0,>=8.18->aisetup) (0.8.3)\n",
      "Requirement already satisfied: asttokens in c:\\users\\soporte\\anaconda3\\lib\\site-packages (from stack_data->ipython<9.0,>=8.18->aisetup) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\soporte\\anaconda3\\lib\\site-packages (from stack_data->ipython<9.0,>=8.18->aisetup) (0.2.2)\n",
      "Installing collected packages: widgetsnbextension, traitlets, requests, prompt_toolkit, jupyterlab_widgets, comm, matplotlib, branca, ipython, folium, ipywidgets, aisetup\n",
      "  Attempting uninstall: widgetsnbextension\n",
      "    Found existing installation: widgetsnbextension 4.0.5\n",
      "    Uninstalling widgetsnbextension-4.0.5:\n",
      "      Successfully uninstalled widgetsnbextension-4.0.5\n",
      "  Attempting uninstall: traitlets\n",
      "    Found existing installation: traitlets 5.7.1\n",
      "    Uninstalling traitlets-5.7.1:\n",
      "      Successfully uninstalled traitlets-5.7.1\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: prompt_toolkit\n",
      "    Found existing installation: prompt-toolkit 3.0.36\n",
      "    Uninstalling prompt-toolkit-3.0.36:\n",
      "      Successfully uninstalled prompt-toolkit-3.0.36\n",
      "  Attempting uninstall: jupyterlab_widgets\n",
      "    Found existing installation: jupyterlab-widgets 3.0.5\n",
      "    Uninstalling jupyterlab-widgets-3.0.5:\n",
      "      Successfully uninstalled jupyterlab-widgets-3.0.5\n",
      "  Attempting uninstall: comm\n",
      "    Found existing installation: comm 0.1.2\n",
      "    Uninstalling comm-0.1.2:\n",
      "      Successfully uninstalled comm-0.1.2\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.8.3\n",
      "    Uninstalling matplotlib-3.8.3:\n",
      "      Successfully uninstalled matplotlib-3.8.3\n",
      "  Attempting uninstall: ipython\n",
      "    Found existing installation: ipython 8.12.0\n",
      "    Uninstalling ipython-8.12.0:\n",
      "      Successfully uninstalled ipython-8.12.0\n",
      "  Attempting uninstall: ipywidgets\n",
      "    Found existing installation: ipywidgets 8.0.4\n",
      "    Uninstalling ipywidgets-8.0.4:\n",
      "      Successfully uninstalled ipywidgets-8.0.4\n",
      "Successfully installed aisetup-0.1.9 branca-0.8.1 comm-0.2.3 folium-0.17.0 ipython-8.37.0 ipywidgets-8.1.7 jupyterlab_widgets-3.0.15 matplotlib-3.10.6 prompt_toolkit-3.0.52 requests-2.32.5 traitlets-5.14.3 widgetsnbextension-4.0.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "conda-repo-cli 1.0.41 requires requests_mock, which is not installed.\n",
      "transformers 2.1.1 requires sentencepiece, which is not installed.\n",
      "conda-repo-cli 1.0.41 requires clyent==1.2.1, but you have clyent 1.2.2 which is incompatible.\n",
      "conda-repo-cli 1.0.41 requires nbformat==5.4.0, but you have nbformat 5.7.0 which is incompatible.\n",
      "conda-repo-cli 1.0.41 requires requests==2.28.1, but you have requests 2.32.5 which is incompatible.\n",
      "jupyter-server 1.13.5 requires pywinpty<2; os_name == \"nt\", but you have pywinpty 2.0.10 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install aisetup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8959f157",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soporte\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:61: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: An OpenAI API key is required to use AI model functions. Please provide the key by calling `authenticate(openai_api_key)` or ensure it is specified in the `.env` file as 'OPENAI_API_KEY'. If you set it in the `.env` file, call `authenticate()` or reload the package to proceed.\n"
     ]
    }
   ],
   "source": [
    "from aisetup import get_llm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "84ca09ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'chat'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m get_llm_response(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhy is the programming language called Python?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Print LLMs response\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\aisetup\\__init__.py:73\u001b[0m, in \u001b[0;36mget_llm_response\u001b[1;34m(prompt)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(prompt, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput must be a string enclosed in quotes.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 73\u001b[0m completion \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m     74\u001b[0m     model\u001b[38;5;241m=\u001b[39mMODEL,\n\u001b[0;32m     75\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     76\u001b[0m         {\n\u001b[0;32m     77\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     78\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a helpful but terse AI assistant who gets straight to the point.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     79\u001b[0m         },\n\u001b[0;32m     80\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt},\n\u001b[0;32m     81\u001b[0m     ],\n\u001b[0;32m     82\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,\n\u001b[0;32m     83\u001b[0m )\n\u001b[0;32m     84\u001b[0m response \u001b[38;5;241m=\u001b[39m completion\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'chat'"
     ]
    }
   ],
   "source": [
    "response = get_llm_response(\"Why is the programming language called Python?\")\n",
    "\n",
    "# Print LLMs response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "daa1016b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from aisetup import print_llm_response\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bf6b8ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Weather API key from the .env file\n",
    "# https://openweathermap.org/price\n",
    "load_dotenv('.env', override=True)\n",
    "api_key = os.getenv('WEATHER_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dbb24609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the latitude value in the 'lat' variable\n",
    "lat = -12.04637   # Palo Alto, CA\n",
    "\n",
    "# Store the longitude value in the 'long' variable\n",
    "lon = -77.042793"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7120498a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f\"https://api.openweathermap.org/data/2.5/forecast?units=metric&cnt=1&lat={lat}&lon={lon}&appid={api_key}\"\n",
    "\n",
    "# Use the get function from the requests library to store the response from the API\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c5e47d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cod': '200', 'message': 0, 'cnt': 1, 'list': [{'dt': 1758002400, 'main': {'temp': 16.21, 'feels_like': 16.03, 'temp_min': 16.21, 'temp_max': 16.29, 'pressure': 1014, 'sea_level': 1014, 'grnd_level': 987, 'humidity': 82, 'temp_kf': -0.08}, 'weather': [{'id': 804, 'main': 'Clouds', 'description': 'overcast clouds', 'icon': '04n'}], 'clouds': {'all': 100}, 'wind': {'speed': 2.19, 'deg': 185, 'gust': 2.08}, 'visibility': 10000, 'pop': 0, 'sys': {'pod': 'n'}, 'dt_txt': '2025-09-16 06:00:00'}], 'city': {'id': 3930376, 'name': 'R√≠mac', 'coord': {'lat': -12.0464, 'lon': -77.0428}, 'country': 'PE', 'population': 0, 'timezone': -18000, 'sunrise': 1757934148, 'sunset': 1757977461}}\n"
     ]
    }
   ],
   "source": [
    "# Take the response from the API (in JSON) and assign it to a Python dictionary\n",
    "data = response.json()\n",
    "\n",
    "# Print\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cc77e23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = data['list'][0]['main']['temp']\n",
    "description = data['list'][0]['weather'][0]['description']\n",
    "wind_speed = data['list'][0]['wind']['speed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8130dde6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 16.21\n",
      "Weather Description: overcast clouds\n",
      "Wind Speed: 2.19\n"
     ]
    }
   ],
   "source": [
    "print(f\"Temperature: {temperature}\")\n",
    "print(f\"Weather Description: {description}\")\n",
    "print(f\"Wind Speed: {wind_speed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "de424e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The temperature is 16.21¬∞C. \n",
      "It is currently overcast clouds,\n",
      "with a wind speed of 2.19m/s.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_string = f\"\"\"The temperature is {temperature}¬∞C. \n",
    "It is currently {description},\n",
    "with a wind speed of {wind_speed}m/s.\n",
    "\"\"\"\n",
    "\n",
    "print(weather_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a2a2663c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'chat'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mBased on the following weather, \u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124msuggest an appropriate outdoor outfit.\u001b[39m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;124mForecast: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweather_string\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Print the LLM response\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m print_llm_response(prompt)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\aisetup\\__init__.py:61\u001b[0m, in \u001b[0;36mprint_llm_response\u001b[1;34m(prompt)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_llm_response\u001b[39m(prompt):\n\u001b[0;32m     58\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This function takes as input a prompt, which must be a string enclosed in quotation marks,\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;124;03m    and passes it to OpenAI's GPT3.5 model. The function then prints the response of the model.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m     llm_response \u001b[38;5;241m=\u001b[39m get_llm_response(prompt)\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28mprint\u001b[39m(llm_response)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\aisetup\\__init__.py:73\u001b[0m, in \u001b[0;36mget_llm_response\u001b[1;34m(prompt)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(prompt, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput must be a string enclosed in quotes.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 73\u001b[0m completion \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m     74\u001b[0m     model\u001b[38;5;241m=\u001b[39mMODEL,\n\u001b[0;32m     75\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     76\u001b[0m         {\n\u001b[0;32m     77\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     78\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a helpful but terse AI assistant who gets straight to the point.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     79\u001b[0m         },\n\u001b[0;32m     80\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt},\n\u001b[0;32m     81\u001b[0m     ],\n\u001b[0;32m     82\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,\n\u001b[0;32m     83\u001b[0m )\n\u001b[0;32m     84\u001b[0m response \u001b[38;5;241m=\u001b[39m completion\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'chat'"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"Based on the following weather, \n",
    "suggest an appropriate outdoor outfit.\n",
    "\n",
    "Forecast: {weather_string}\n",
    "\"\"\"\n",
    "\n",
    "# Print the LLM response\n",
    "print_llm_response(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257e3cd7",
   "metadata": {},
   "source": [
    "## APIs to use AI models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8e93fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd11f34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the OpenAI API key from the .env file\n",
    "load_dotenv('.env', override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "client = OpenAI(api_key = openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d1271912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_response(prompt):\n",
    "     completion = client.chat.completions.create(\n",
    "     model=\"gpt-4o-mini\",\n",
    "     messages=[\n",
    "      {\n",
    "       \"role\": \"system\",\n",
    "       \"content\": \"You are a helpful but terse AI assistant who gets straight to the point.\",\n",
    "       },\n",
    "      {\"role\": \"user\", \"content\": prompt},\n",
    "      ],\n",
    "     temperature=0.0,\n",
    "     )\n",
    "     response = completion.choices[0].message.content\n",
    "     return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb803a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aisetup import authenticate, print_llm_response, get_llm_response\n",
    "\n",
    "authenticate(\"YOUR API KEY HERE\")\n",
    "\n",
    "# Print the LLM response\n",
    "print_llm_response(\"What is the capital of France\")\n",
    "\n",
    "# Store the LLM response as a variable and then print\n",
    "response = get_llm_response(\"What is the capital of France\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5cf00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aisetup import authenticate, print_llm_response, get_llm_response\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv('.env', override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "authenticate(openai_api_key)\n",
    "\n",
    "# Print the LLM response\n",
    "print_llm_response(\"What is the capital of France\")\n",
    "\n",
    "# Store the LLM response as a variable and then print\n",
    "response = print_llm_response(\"What is the capital of France\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d807eabf",
   "metadata": {},
   "source": [
    "Work on projects, like ones that might involve:\n",
    "\n",
    "    * Downloading a webpage\n",
    "    * Conducting a web search\n",
    "    * Extracting text from a PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a988f246",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
